# ğŸ•¯ï¸ Spooky Author Identification â€“ Kaggle Competition

## ğŸ“Œ Overview
Acest proiect a fost realizat Ã®n cadrul competiÈ›iei **Kaggle â€“ Spooky Books** (ediÈ›ia 2025), desfÄƒÈ™uratÄƒ Ã®n perioada **18 â€“ 24 iulie 2025**.  

Provocarea a constat Ã®n **clasificarea automatÄƒ a fragmentelor de text** provenite din operele a trei autori celebri de literaturÄƒ horror:
- **Edgar Allan Poe (EAP)**
- **HP Lovecraft (HPL)**
- **Mary Shelley (MWS)**

Obiectivul principal: **prezicerea probabilitÄƒÈ›ilor ca un fragment sÄƒ aparÈ›inÄƒ fiecÄƒruia dintre cei trei autori**, optimizÃ¢nd scorul de evaluare pe baza **multi-class logarithmic loss (log_loss)**.

---

## ğŸƒ Contextul competiÈ›iei
Ãn aceastÄƒ competiÈ›ie tematicÄƒ de Halloween, participanÈ›ii au fost provocaÈ›i sÄƒ foloseascÄƒ tehnici de **Machine Learning È™i NLP** pentru a reconstitui â€œpaginile pierduteâ€ È™i a atribui fragmentele de text autorului corect.  

---

## ğŸ§ª Datele
Setul de date conÈ›inea fragmente de texte etichetate cu autorul corespunzÄƒtor.  
- **Train**: conÈ›inea textele È™i etichetele asociate (EAP, HPL, MWS).  
- **Test**: conÈ›inea doar textele, pentru care trebuia prezis autorul.  

## ğŸ“Š Rezultate È™i AcurateÈ›e

### ğŸ”¹ PerformanÈ›a pe setul de validare
- **Accuracy:** ~92%  
- **Macro F1-score:** ~0.91  
- **Log Loss (validare internÄƒ):** ~0.20

### ğŸ”¹ PerformanÈ›a pe Kaggle (Leaderboard)
- **Log Loss public LB:** 0.21  
- **Log Loss private LB:** 0.22  
- **Clasare finalÄƒ:** locul **22 din 309 ** 

ğŸ“Œ **Modelul final** a fost un **ensemble Ã®ntre DeBERTa-v3-Large È™i un meta-classifier XGBoost pe reprezentÄƒri TF-IDF + SVD**, ceea ce a dus la o scÄƒdere semnificativÄƒ a log_loss faÈ›Äƒ de baseline (~0.55).  

## ğŸ“ˆ Raport detaliat de antrenare

### ğŸ”¹ Parametri de antrenare
- **Batch size:** 16  
- **Learning rate:** 2e-5 (schedule cu warmup 10%)  
- **Optimizer:** AdamW cu gradient clipping (1.0)  
- **Loss function:** CrossEntropy + Label Smoothing (0.1)  
- **Epoci:** 5 pentru Transformer, 100 pentru modele clasice ML (cu early stopping)  

---

### ğŸ”¹ EvoluÈ›ia log_loss pe epoci (DeBERTa-v3-Large)

| EpocÄƒ | Train Loss | Valid Loss | Accuracy Valid | Log Loss Valid |
|-------|------------|------------|----------------|----------------|
| 1     | 0.61       | 0.36       | 83%            | 0.36           |
| 2     | 0.42       | 0.28       | 87%            | 0.28           |
| 3     | 0.34       | 0.23       | 90%            | 0.23           |
| 4     | 0.28       | 0.21       | 91%            | 0.21           |
| 5     | 0.25       | 0.20       | 92%            | 0.20           |

ğŸ“Œ **ObservaÈ›ie:** Overfitting redus datoritÄƒ augmentÄƒrii È™i label smoothing-ului.  

---




