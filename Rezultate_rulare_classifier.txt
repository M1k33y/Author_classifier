[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /root/nltk_data...
[nltk_data]   Package averaged_perceptron_tagger is already up-to-
[nltk_data]       date!
[nltk_data] Downloading package wordnet to /root/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /root/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
🔄 Augmentez 1500 exemple cu ContextualWordEmbsAug...
100%|██████████| 94/94 [34:22<00:00, 21.94s/it]
🔄 Augmentez 1500 exemple cu SynonymAug...

100%|██████████| 1500/1500 [00:01<00:00, 1185.68it/s]
✅ Dataset extins: 53952 exemple după augmentare dublă.
✅ Augmentările au fost salvate în 'augmented_texts.csv'

🔁 Fold 1/5
The following layers were not sharded: encoder.layer.*.output.LayerNorm.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, embeddings.word_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.intermediate.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.token_type_embeddings.weight
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/tmp/ipython-input-2-2174834867.py:264: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
  0%|          | 0/1214 [00:00<?, ?it/s]/tmp/ipython-input-2-2174834867.py:276: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 1 | Fold 1 Loss: 0.6610
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 2 | Fold 1 Loss: 0.4054
100%|██████████| 1214/1214 [07:09<00:00,  2.82it/s]
Epoch 3 | Fold 1 Loss: 0.3225
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 4 | Fold 1 Loss: 0.2690

🔁 Fold 2/5
The following layers were not sharded: encoder.layer.*.output.LayerNorm.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, embeddings.word_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.intermediate.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.token_type_embeddings.weight
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/tmp/ipython-input-2-2174834867.py:264: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
  0%|          | 0/1214 [00:00<?, ?it/s]/tmp/ipython-input-2-2174834867.py:276: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 1 | Fold 2 Loss: 0.7124
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 2 | Fold 2 Loss: 0.4070
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 3 | Fold 2 Loss: 0.3223
100%|██████████| 1214/1214 [07:09<00:00,  2.82it/s]
Epoch 4 | Fold 2 Loss: 0.2701

🔁 Fold 3/5
The following layers were not sharded: encoder.layer.*.output.LayerNorm.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, embeddings.word_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.intermediate.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.token_type_embeddings.weight
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/tmp/ipython-input-2-2174834867.py:264: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
  0%|          | 0/1214 [00:00<?, ?it/s]/tmp/ipython-input-2-2174834867.py:276: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 1 | Fold 3 Loss: 0.6727
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 2 | Fold 3 Loss: 0.4080
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 3 | Fold 3 Loss: 0.3182
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 4 | Fold 3 Loss: 0.2690

🔁 Fold 4/5
The following layers were not sharded: encoder.layer.*.output.LayerNorm.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, embeddings.word_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.intermediate.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.token_type_embeddings.weight
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/tmp/ipython-input-2-2174834867.py:264: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
  0%|          | 0/1214 [00:00<?, ?it/s]/tmp/ipython-input-2-2174834867.py:276: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
100%|██████████| 1214/1214 [07:11<00:00,  2.82it/s]
Epoch 1 | Fold 4 Loss: 0.6044
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 2 | Fold 4 Loss: 0.3893
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 3 | Fold 4 Loss: 0.3073
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 4 | Fold 4 Loss: 0.2611

🔁 Fold 5/5
The following layers were not sharded: encoder.layer.*.output.LayerNorm.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, embeddings.word_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.intermediate.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.token_type_embeddings.weight
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/tmp/ipython-input-2-2174834867.py:264: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
  0%|          | 0/1214 [00:00<?, ?it/s]/tmp/ipython-input-2-2174834867.py:276: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 1 | Fold 5 Loss: 0.6090
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 2 | Fold 5 Loss: 0.3879
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 3 | Fold 5 Loss: 0.3042
100%|██████████| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 4 | Fold 5 Loss: 0.2599
📦 Fold 1/3
📦 Fold 2/3
📦 Fold 3/3
📉 Ensemble Validation Log Loss (XGBoost): 0.07552
✅ Final submission saved as 'submission.csv'
📦 Dataset final: 53952 instanțe antrenare după augmentare.
📉 Scor final VALIDATION LOG-LOSS: 0.07552