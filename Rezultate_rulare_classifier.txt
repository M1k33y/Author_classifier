[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /root/nltk_data...
[nltk_data]   Package averaged_perceptron_tagger is already up-to-
[nltk_data]       date!
[nltk_data] Downloading package wordnet to /root/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /root/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
ðŸ”„ Augmentez 1500 exemple cu ContextualWordEmbsAug...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 94/94 [34:22<00:00, 21.94s/it]
ðŸ”„ Augmentez 1500 exemple cu SynonymAug...

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:01<00:00, 1185.68it/s]
âœ… Dataset extins: 53952 exemple dupÄƒ augmentare dublÄƒ.
âœ… AugmentÄƒrile au fost salvate Ã®n 'augmented_texts.csv'

ðŸ” Fold 1/5
The following layers were not sharded: encoder.layer.*.output.LayerNorm.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, embeddings.word_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.intermediate.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.token_type_embeddings.weight
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/tmp/ipython-input-2-2174834867.py:264: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
  0%|          | 0/1214 [00:00<?, ?it/s]/tmp/ipython-input-2-2174834867.py:276: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 1 | Fold 1 Loss: 0.6610
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 2 | Fold 1 Loss: 0.4054
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:09<00:00,  2.82it/s]
Epoch 3 | Fold 1 Loss: 0.3225
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 4 | Fold 1 Loss: 0.2690

ðŸ” Fold 2/5
The following layers were not sharded: encoder.layer.*.output.LayerNorm.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, embeddings.word_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.intermediate.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.token_type_embeddings.weight
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/tmp/ipython-input-2-2174834867.py:264: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
  0%|          | 0/1214 [00:00<?, ?it/s]/tmp/ipython-input-2-2174834867.py:276: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 1 | Fold 2 Loss: 0.7124
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 2 | Fold 2 Loss: 0.4070
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 3 | Fold 2 Loss: 0.3223
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:09<00:00,  2.82it/s]
Epoch 4 | Fold 2 Loss: 0.2701

ðŸ” Fold 3/5
The following layers were not sharded: encoder.layer.*.output.LayerNorm.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, embeddings.word_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.intermediate.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.token_type_embeddings.weight
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/tmp/ipython-input-2-2174834867.py:264: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
  0%|          | 0/1214 [00:00<?, ?it/s]/tmp/ipython-input-2-2174834867.py:276: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 1 | Fold 3 Loss: 0.6727
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 2 | Fold 3 Loss: 0.4080
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 3 | Fold 3 Loss: 0.3182
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 4 | Fold 3 Loss: 0.2690

ðŸ” Fold 4/5
The following layers were not sharded: encoder.layer.*.output.LayerNorm.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, embeddings.word_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.intermediate.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.token_type_embeddings.weight
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/tmp/ipython-input-2-2174834867.py:264: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
  0%|          | 0/1214 [00:00<?, ?it/s]/tmp/ipython-input-2-2174834867.py:276: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:11<00:00,  2.82it/s]
Epoch 1 | Fold 4 Loss: 0.6044
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 2 | Fold 4 Loss: 0.3893
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 3 | Fold 4 Loss: 0.3073
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 4 | Fold 4 Loss: 0.2611

ðŸ” Fold 5/5
The following layers were not sharded: encoder.layer.*.output.LayerNorm.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.output.LayerNorm.bias, embeddings.position_embeddings.weight, embeddings.word_embeddings.weight, pooler.dense.bias, encoder.layer.*.attention.output.dense.weight, encoder.layer.*.intermediate.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, embeddings.token_type_embeddings.weight
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/tmp/ipython-input-2-2174834867.py:264: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
  0%|          | 0/1214 [00:00<?, ?it/s]/tmp/ipython-input-2-2174834867.py:276: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 1 | Fold 5 Loss: 0.6090
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 2 | Fold 5 Loss: 0.3879
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 3 | Fold 5 Loss: 0.3042
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [07:10<00:00,  2.82it/s]
Epoch 4 | Fold 5 Loss: 0.2599
ðŸ“¦ Fold 1/3
ðŸ“¦ Fold 2/3
ðŸ“¦ Fold 3/3
ðŸ“‰ Ensemble Validation Log Loss (XGBoost): 0.07552
âœ… Final submission saved as 'submission.csv'
ðŸ“¦ Dataset final: 53952 instanÈ›e antrenare dupÄƒ augmentare.
ðŸ“‰ Scor final VALIDATION LOG-LOSS: 0.07552